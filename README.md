# ğŸ¤– ReflectDiffu-Reproduce

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> **Reproduction implementation for:** *ReflectDiffu: Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework*

This repository provides a complete implementation for training and evaluating the ReflectDiffu model, which generates empathetic responses by combining emotion-intent contagion with reinforcement learning and diffusion frameworks.

## ğŸš€ Quick Start

## Final Result Fast Review
You can review current the best result in
```
./output/eval_logs/
```

### Evaluate Pre-trained Model

Run the best pre-trained model with comprehensive evaluation metrics:

```bash
python display.py --model_path output/best_model/best_model.pt
```

**Output:** Evaluation results will be saved in `output/eval_logs/`

**Metrics Evaluated:**
- ğŸ“Š **Relevance**: BLEU-1/2/3/4, BARTScore
- ğŸ¯ **Informativeness**: Perplexity (PPL), Distinct-1/2

### Custom Evaluation

```bash
# Evaluate with custom parameters
python display.py --model_path path/to/model.pt --data_path dataset/test.pkl --max_samples 200

# Use specific device
python display.py --model_path output/best_model/best_model.pt --device cuda
```

## ğŸ—ï¸ Training Setup

### 1. ğŸ“ Data Preparation

Place your data files in the `dataset/` directory:

```
dataset/
â”œâ”€â”€ emotion_labels_user_response.pkl  # Training data
â””â”€â”€ emotion_labels_test.pkl           # Test data
```

**Data Format:**
```python
[
    [
        [user_data, response_data]
    ],
    [
        [user_data1, response_data1]
    ],
    # ... more conversation pairs
]
```

**Data Structure:**
- `user_data`: `[(word1, <em>), (word2, <noem>), ...]`
- `response_data`: `[(word1, <em>), (word2, <noem>), ...]`

### 2. ğŸ§  EmpHi Intent Prediction Setup

Download the required EmpHi models and reflect-Diffu best models from Google Drive and organize as follows:

```
pre-trained/
â”œâ”€â”€ intent_prediction/
â”‚   â””â”€â”€ paras.pkl          # Intent prediction parameters
â””â”€â”€ model/
    â””â”€â”€ model              # Pre-trained EmpHi model

output
â””â”€â”€ best_model/
    â””â”€â”€ best_model.pt      # trained best model
```

### 3. ğŸš‚ Start Training

```bash
python train.py
```

## ğŸ“Š Evaluation Metrics

### Relevance Metrics
- **BLEU-1/2/3/4**: N-gram overlap with reference responses
- **BARTScore**: Semantic similarity using BART model
- **Brevity Penalty**: Length normalization factor

### Informativeness Metrics
- **Perplexity (PPL)**: Language model confidence (lower is better)
- **Distinct-1/2**: Lexical diversity at unigram/bigram level (higher is better)

## ğŸ› ï¸ Dependencies

Key requirements:
- Python 3.10
- PyTorch 2.0+
- transformers
- sacrebleu
- torcheval (optional, for optimized perplexity computation)

Install dependencies:
```bash
pip install -r requirement
```

## ğŸ“ Usage Examples

### Basic Evaluation
```bash
python display.py --model_path output/best_model/best_model.pt
```

### Training from Scratch
```bash
# Ensure data is in dataset/ and pre-trained models are in pre-trained/
python train.py
```

### Custom Evaluation with Specific Parameters
```bash
python display.py \
    --model_path checkpoints/epoch_10.pt \
    --data_path dataset/emotion_labels_test.pkl \
    --max_samples 500 \
    --device cuda
```

## ğŸ“‚ Project Structure

```
ReflectDiffu-Reproduce/
â”œâ”€â”€ src/                           # Source code modules
â”‚   â”œâ”€â”€ emotion_contagion/         # Emotion contagion components
â”‚   â”œâ”€â”€ intent_twice/              # Intent processing modules
â”‚   â””â”€â”€ era/                       # ERA components
â”œâ”€â”€ evaluation/                    # Evaluation scripts
â”‚   â”œâ”€â”€ relevance_evaluation.py    # BLEU & BARTScore evaluation
â”‚   â””â”€â”€ informativeness.py         # Perplexity & Distinct-n evaluation
â”œâ”€â”€ dataset/                       # Training and test data
â”œâ”€â”€ pre-trained/                   # Pre-trained models
â”œâ”€â”€ output/                        # Model outputs and logs
â”‚   â”œâ”€â”€ best_model/               # Best trained model
â”‚   â””â”€â”€ eval_logs/                # Evaluation results
â”œâ”€â”€ display.py                     # Main evaluation script
â”œâ”€â”€ train.py                       # Training script
â””â”€â”€ README.md                      # This file
```

