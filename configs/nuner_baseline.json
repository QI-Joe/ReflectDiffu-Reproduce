{
  "bert_model": "roberta-base",
  "llm_model_path": "zai-org/chatglm-6b",
  "num_labels": 3,
  "tagging_scheme": "BIO",
  "max_length": 512,
  "frozen_layers": 6,
  "learning_rate": 3e-05,
  "batch_size": 48,
  "gradient_accumulation_steps": 1,
  "optimizer": "adamw",
  "adam_beta1": 0.9,
  "adam_beta2": 0.999,
  "adam_epsilon": 1e-08,
  "weight_decay": 0.01,
  "scheduler_type": "linear",
  "warmup_ratio": 0.1,
  "num_epochs": 10,
  "early_stopping_patience": 3,
  "save_strategy": "epoch",
  "evaluation_strategy": "epoch",
  "dropout_rate": 0.1,
  "label_smoothing": 0.0,
  "hidden_size": 768,
  "use_crf": true,
  "crf_lr_multiplier": 1.0,
  "encoder_lr": 3e-05,
  "head_lr": 5e-05,
  "data_dir": "./dataset",
  "cache_dir": "./cache",
  "models_dir": "./models",
  "train_ratio": 0.8,
  "valid_ratio": 0.1,
  "test_ratio": 0.1,
  "max_train_samples": null,
  "max_eval_samples": null,
  "oversampling_ratio": 1.5,
  "output_dir": "./checkpoints",
  "logging_dir": "./logs",
  "device": "auto",
  "fp16": false,
  "dataloader_num_workers": 4,
  "dataloader_pin_memory": true,
  "log_level": "INFO",
  "report_to": ["tensorboard"],
  "logging_steps": 50,
  "save_steps": 500,
  "eval_steps": 500,
  "metric_for_best_model": "eval_f1",
  "greater_is_better": true,
  "eval_batch_size": 32,
  "prediction_loss_only": false,
  "seed": 42,
  "data_seed": 42,
  "max_grad_norm": 1.0,
  "ignore_index": -100,
  "class_weights": null,
  "crf_reduction": "mean",
  "debug_mode": false,
  "fast_dev_run": false,
  "overfit_batches": 0,
  "experiment_name": "era_nuner_baseline",
  "notes": "NuNER baseline configuration following EAR.md specifications"
}