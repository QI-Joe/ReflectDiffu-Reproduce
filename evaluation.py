import argparse
import os
import torch
import torch.nn.functional as F
from torch import nn
import time
import json
from pathlib import Path
from dataclasses import dataclass
from typing import Optional, List, Dict, Any
import logging

from evaluation import ReflectDiffuEvaluator, EvaluationResults, EvaluationSample
EVALUATION_AVAILABLE = True

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class EvaluationConfig:
    """è¯„ä¼°é…ç½®ç±» - æŒ‰ç…§è®¾è®¡æ–‡æ¡£å®ç°"""
    # è¯„ä¼°é¢‘ç‡æ§åˆ¶
    eval_every_epochs: int = 1          # æ¯Nä¸ªepochè¯„ä¼°ä¸€æ¬¡
    eval_every_steps: Optional[int] = None  # æ¯Nä¸ªstepè¯„ä¼°ä¸€æ¬¡(ä¼˜å…ˆçº§é«˜)
    eval_at_end: bool = True           # è®­ç»ƒç»“æŸæ—¶è¯„ä¼°
    
    # è¯„ä¼°æ•°æ®æ§åˆ¶
    eval_data_path: Optional[str] = None  # è¯„ä¼°æ•°æ®è·¯å¾„
    max_eval_samples: int = 100        # æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°
    
    # ç”Ÿæˆå‚æ•°
    max_gen_length: int = 32           # æœ€å¤§ç”Ÿæˆé•¿åº¦
    use_pointer: bool = False          # æ˜¯å¦ä½¿ç”¨æŒ‡é’ˆç½‘ç»œ
    temperature: float = 1.0           # æ¸©åº¦å‚æ•°
    top_k: int = 0                     # Top-k é‡‡æ ·
    top_p: float = 1.0                 # Top-p é‡‡æ ·
    
    # æŒ‡æ ‡è®¡ç®—
    compute_bleu: bool = True          # è®¡ç®—BLEUæŒ‡æ ‡
    compute_bart_score: bool = True    # è®¡ç®—BARTScore
    bart_checkpoint: str = "facebook/bart-large-cnn"
    
    # è¾“å‡ºæ§åˆ¶
    save_results: bool = True          # ä¿å­˜ç»“æœ
    results_dir: str = "evaluation_results"
    log_examples: bool = True          # è®°å½•ç”Ÿæˆæ ·ä¾‹
    num_examples: int = 5              # è®°å½•æ ·ä¾‹æ•°é‡
    
    # æ€§èƒ½æ§åˆ¶
    eval_batch_size: int = 1           # è¯„ä¼°æ‰¹é‡å¤§å°
    disable_tqdm: bool = False         # ç¦ç”¨è¿›åº¦æ¡
    
    def __post_init__(self):
        """é…ç½®éªŒè¯"""
        if self.eval_every_epochs <= 0:
            raise ValueError("eval_every_epochs must be positive")
        if self.eval_every_steps is not None and self.eval_every_steps <= 0:
            raise ValueError("eval_every_steps must be positive")
        if self.max_eval_samples <= 0:
            raise ValueError("max_eval_samples must be positive")


class EvaluationLogger:
    """è¯„ä¼°ç»“æœæ—¥å¿—ç®¡ç†å™¨"""
    
    def __init__(self, log_dir: str):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.metrics_file = self.log_dir / "metrics.jsonl"
        self.examples_file = self.log_dir / "examples.txt"
        
    def log_metrics(self, results: "EvaluationResults", epoch: int, step: Optional[int] = None):
        """è®°å½•æŒ‡æ ‡åˆ°æ–‡ä»¶"""
        log_entry = {
            "epoch": epoch,
            "step": step,
            "timestamp": time.time(),
            "metrics": {
                "BLEU-1": results.bleu_1,
                "BLEU-2": results.bleu_2,
                "BLEU-3": results.bleu_3,
                "BLEU-4": results.bleu_4,
                "BLEU": results.bleu_overall,
                "BARTScore": results.bart_score,
                "BP": results.brevity_penalty,
                "num_samples": results.num_samples,
                "avg_hyp_length": results.avg_hyp_length,
                "avg_ref_length": results.avg_ref_length,
                "generation_time": results.generation_time
            }
        }
        
        with open(self.metrics_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
            
        logger.info(f"Epoch {epoch}: BLEU-4={results.bleu_4:.2f}%, BARTScore={results.bart_score:.4f}")
    
    def log_examples(self, hyps: List[str], refs: List[str], num_examples: int, epoch: int):
        """è®°å½•ç”Ÿæˆæ ·ä¾‹"""
        with open(self.examples_file, "a", encoding="utf-8") as f:
            f.write(f"\n=== Epoch {epoch} Examples ===\n")
            for i in range(min(num_examples, len(hyps), len(refs))):
                f.write(f"Example {i+1}:\n")
                f.write(f"  Hypothesis: {hyps[i]}\n")
                f.write(f"  Reference:  {refs[i]}\n")
                f.write("\n")


class TrainingEvaluator:
    """è®­ç»ƒæœŸé—´çš„è¯„ä¼°ç®¡ç†å™¨ - æŒ‰ç…§è®¾è®¡æ–‡æ¡£å®ç°"""
    
    def __init__(self, model, config: EvaluationConfig):
        """
        åˆå§‹åŒ–è®­ç»ƒè¯„ä¼°å™¨
        
        Args:
            model: ReflectDiffu æ¨¡å‹å®ä¾‹
            config: è¯„ä¼°é…ç½®
        """
        if not EVALUATION_AVAILABLE:
            raise RuntimeError("Evaluation module not available. Cannot create TrainingEvaluator.")
        
        self.model = model
        self.config = config
        self.evaluator = None  # ReflectDiffuEvaluator å®ä¾‹ï¼Œå»¶è¿Ÿåˆå§‹åŒ–
        self.eval_data = None  # è¯„ä¼°æ•°æ®ï¼Œå»¶è¿ŸåŠ è½½
        self.logger = None     # æ—¥å¿—è®°å½•å™¨ï¼Œå»¶è¿Ÿåˆå§‹åŒ–
        self.initialized = False
        
        # è¿½è¸ªçŠ¶æ€
        self.last_eval_epoch = -1
        self.last_eval_step = -1
        self.best_bleu4 = 0.0
        self.best_bart_score = float('-inf')
        
    def initialize(self):
        """å»¶è¿Ÿåˆå§‹åŒ–è¯„ä¼°å™¨ï¼Œå¤ç”¨è®­ç»ƒæ¨¡å‹ç»„ä»¶"""
        if self.initialized:
            return
            
        logger.info("Initializing training evaluator...")
        
        # 1. åˆ›å»ºè¯„ä¼°å™¨ï¼Œç›´æ¥å¤ç”¨è®­ç»ƒæ¨¡å‹ç»„ä»¶
        self.evaluator = ReflectDiffuEvaluator(
            encoder=self.model.encoder,           # ç›´æ¥å¤ç”¨è®­ç»ƒæ¨¡å‹çš„ç¼–ç å™¨
            it_module=self.model.it_module,       # ç›´æ¥å¤ç”¨Intent Twiceæ¨¡å—
            decoder=self.model.decoder_with_loss, # ç›´æ¥å¤ç”¨è§£ç å™¨
            device=self.model.device,             # ä½¿ç”¨ç›¸åŒè®¾å¤‡
            bart_checkpoint=self.config.bart_checkpoint
        )
        
        # 2. åŠ è½½è¯„ä¼°æ•°æ®
        self._load_eval_data()
        
        # 3. åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
        if self.config.save_results:
            self.logger = EvaluationLogger(self.config.results_dir)
        
        self.initialized = True
        logger.info("Training evaluator initialized successfully")
            
    
    def _load_eval_data(self):
        if self.config.eval_data_path:
            # ä½¿ç”¨ä¸“é—¨çš„è¯„ä¼°æ•°æ®
            eval_raw_data = self.model.dp.load_data(self.config.eval_data_path)
            logger.info(f"Loaded evaluation data from {self.config.eval_data_path}")
        else:
            # å¤ç”¨è®­ç»ƒæ•°æ®ä½œä¸ºè¯„ä¼°æ•°æ®
            eval_raw_data = self.model.raw_data
            logger.info("Using training data for evaluation")
        
        # å‡†å¤‡è¯„ä¼°æ ·æœ¬
        self.eval_data = self.evaluator.prepare_evaluation_data()
        
        logger.info(f"Prepared {len(self.eval_data)} evaluation samples")
            
    
    def should_evaluate(self, epoch: int, step: Optional[int] = None) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›è¡Œè¯„ä¼°"""
        if not self.initialized or not self.eval_data:
            return False
        
        # ä¼˜å…ˆæ£€æŸ¥æ­¥æ•°è¯„ä¼°
        if self.config.eval_every_steps is not None and step is not None:
            if step > 0 and step % self.config.eval_every_steps == 0 and step != self.last_eval_step:
                return True
        
        # æ£€æŸ¥è½®æ¬¡è¯„ä¼°
        if epoch > 0 and epoch % self.config.eval_every_epochs == 0 and epoch != self.last_eval_epoch:
            return True
        
        return False
    
    def evaluate(self, **kwargs) -> Optional["EvaluationResults"]:
        """æ‰§è¡Œè¯„ä¼°ï¼Œè¿”å›ç»“æœ"""
        if not self.initialized or not self.eval_data:
            logger.warning("Evaluator not initialized or no evaluation data")
            return None
        
        try:
            logger.info(f"Starting evaluation on {len(self.eval_data)} samples...")
            
            # åˆå¹¶ç”Ÿæˆå‚æ•°
            generation_params = {
                'max_len': self.config.max_gen_length,
                'use_pointer': self.config.use_pointer,
                'temperature': self.config.temperature,
                'top_k': self.config.top_k,
                'top_p': self.config.top_p,
                'batch_size_metrics': self.config.eval_batch_size,
                **kwargs  # å…è®¸ä¸´æ—¶è¦†ç›–å‚æ•°
            }
            
            # æ‰§è¡Œè¯„ä¼°
            results = self.evaluator.evaluate(self.eval_data, **generation_params)
            
            # æ›´æ–°æœ€ä½³ç»“æœ
            if results.bleu_4 > self.best_bleu4:
                self.best_bleu4 = results.bleu_4
            if results.bart_score > self.best_bart_score:
                self.best_bart_score = results.bart_score
            
            logger.info(f"Evaluation completed: BLEU-4={results.bleu_4:.2f}%, BARTScore={results.bart_score:.4f}")
            return results
            
        except Exception as e:
            logger.error(f"Evaluation failed: {e}")
            return None
    
    def log_results(self, results: "EvaluationResults", epoch: int, step: Optional[int] = None):
        """è®°å½•è¯„ä¼°ç»“æœ"""
        if results is None:
            return
            
        # æ›´æ–°è¿½è¸ªçŠ¶æ€
        if step is not None:
            self.last_eval_step = step
        self.last_eval_epoch = epoch
        
        # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
        if self.logger:
            self.logger.log_metrics(results, epoch, step)
            
            # è®°å½•ç”Ÿæˆæ ·ä¾‹
            if self.config.log_examples:
                try:
                    # ç”Ÿæˆå°‘é‡æ ·ä¾‹ç”¨äºæ—¥å¿—è®°å½•
                    hyps, refs = self.evaluator.inference_engine.batch_generate(
                        self.eval_data[:self.config.num_examples],
                        max_len=self.config.max_gen_length,
                        use_pointer=self.config.use_pointer
                    )
                    self.logger.log_examples(hyps, refs, self.config.num_examples, epoch)
                except Exception as e:
                    logger.warning(f"Failed to log examples: {e}")
        
        # æ§åˆ¶å°è¾“å‡º
        self._print_results(results, epoch, step)
    
    def _print_results(self, results: "EvaluationResults", epoch: int, step: Optional[int] = None):
        """æ§åˆ¶å°è¾“å‡ºè¯„ä¼°ç»“æœ"""
        step_info = f", Step {step}" if step is not None else ""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š Evaluation Results - Epoch {epoch}{step_info}")
        print(f"{'='*60}")
        print(f"Samples:      {results.num_samples}")
        print(f"Gen Time:     {results.generation_time:.2f}s")
        print(f"{'='*60}")
        print(f"BLEU Metrics:")
        print(f"  BLEU-1:     {results.bleu_1:.2f}%")
        print(f"  BLEU-2:     {results.bleu_2:.2f}%")
        print(f"  BLEU-3:     {results.bleu_3:.2f}%")
        print(f"  BLEU-4:     {results.bleu_4:.2f}% (Best: {self.best_bleu4:.2f}%)")
        print(f"  BLEU:       {results.bleu_overall:.2f}")
        print(f"  BP:         {results.brevity_penalty:.3f}")
        print(f"{'='*60}")
        print(f"BARTScore:    {results.bart_score:.4f} (Best: {self.best_bart_score:.4f})")
        print(f"{'='*60}")
        print(f"Length Stats:")
        print(f"  Avg Hyp:    {results.avg_hyp_length:.1f} words")
        print(f"  Avg Ref:    {results.avg_ref_length:.1f} words")
        print(f"{'='*60}\n")
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–è¯„ä¼°å™¨çŠ¶æ€ä¿¡æ¯"""
        return {
            "initialized": self.initialized,
            "num_eval_samples": len(self.eval_data) if self.eval_data else 0,
            "last_eval_epoch": self.last_eval_epoch,
            "last_eval_step": self.last_eval_step,
            "best_bleu4": self.best_bleu4,
            "best_bart_score": self.best_bart_score,
            "config": self.config
        }